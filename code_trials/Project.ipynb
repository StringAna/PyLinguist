{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder with the name of data\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nfor i in range(10):  # First digit\\n    for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\ndef count_distinct_states(matrix):\\n    coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ndef remove_spaces_and_punctuation(s):\\n    r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nimport math\\n\\ndef is_prime(n):\\n    # Check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nclass String:\\n    def __init__(self, string...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        English_code\n",
       "0  \\nfor i in range(10):  # First digit\\n    for ...\n",
       "1  \\ndef count_distinct_states(matrix):\\n    coun...\n",
       "2  \\ndef remove_spaces_and_punctuation(s):\\n    r...\n",
       "3  \\nimport math\\n\\ndef is_prime(n):\\n    # Check...\n",
       "4  \\nclass String:\\n    def __init__(self, string..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from Hugging Face Datasets and store the output column after extracting the python code into a csv file \n",
    "ds = load_dataset(\"jtatman/python-code-dataset-500k\")\n",
    "ds = pd.DataFrame(ds['train'])\n",
    "ds = ds.drop(['instruction', 'system'], axis=1)\n",
    "ds['English_code'] = ds['output'].apply(lambda x: re.search(r'```python(.*?)```', x, re.DOTALL).group(1) if re.search(r'```python(.*?)```', x, re.DOTALL) else None)\n",
    "ds = ds.drop(['output'], axis=1)\n",
    "ds.to_csv('data/python_code_dataset.csv', index=False)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 50 unprocessed items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating code: 100%|██████████| 50/50 [06:35<00:00,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed! Results saved to: data/google_code_translations.csv\n",
      "\n",
      "Processing completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, max_rows=None):\n",
    "        self.input_path = 'data/python_code_dataset.csv'\n",
    "        self.output_path = 'data/google_code_translations.csv'\n",
    "        self.checkpoint_path = 'data/translation_checkpoint.json'\n",
    "        self.keywords_path = 'data/segregated_data.csv'\n",
    "        self.batch_size = 5\n",
    "        self.source_lang = 'en'\n",
    "        self.target_lang = 'hi'\n",
    "        self.sleep_time = 0.2\n",
    "        self.max_retries = 3\n",
    "        self.max_rows = max_rows\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manages saving and loading of translation progress\"\"\"\n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.processed_indices = set()\n",
    "        self._load_checkpoint()\n",
    "\n",
    "    def _load_checkpoint(self) -> None:\n",
    "        \"\"\"Load existing checkpoint if available\"\"\"\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            try:\n",
    "                with open(self.checkpoint_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                self.processed_indices = set(data.get('processed_indices', []))\n",
    "                print(f\"Loaded checkpoint with {len(self.processed_indices)} processed items\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint: {str(e)}\")\n",
    "                self.processed_indices = set()\n",
    "\n",
    "    def save_checkpoint(self) -> None:\n",
    "        \"\"\"Save current progress to checkpoint file\"\"\"\n",
    "        try:\n",
    "            data = {'processed_indices': list(self.processed_indices)}\n",
    "            with open(self.checkpoint_path, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving checkpoint: {str(e)}\")\n",
    "\n",
    "    def mark_processed(self, index: int) -> None:\n",
    "        \"\"\"Mark an item as processed and save checkpoint\"\"\"\n",
    "        self.processed_indices.add(index)\n",
    "        self.save_checkpoint()\n",
    "\n",
    "    def is_processed(self, index: int) -> bool:\n",
    "        \"\"\"Check if an item has been processed\"\"\"\n",
    "        return index in self.processed_indices\n",
    "\n",
    "    def get_unprocessed_indices(self, total_items: int) -> List[int]:\n",
    "        \"\"\"Get list of indices that haven't been processed yet\"\"\"\n",
    "        return [i for i in range(total_items) if not self.is_processed(i)]\n",
    "\n",
    "class KeywordManager:\n",
    "    \"\"\"Manages programming keyword translations\"\"\"\n",
    "    def __init__(self, keywords_path: str):\n",
    "        self.keywords_path = keywords_path\n",
    "        self.keywords = self._load_keywords()\n",
    "        self._add_special_cases()\n",
    "\n",
    "    def _load_keywords(self) -> Dict[str, str]:\n",
    "        \"\"\"Load keyword translations from file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.keywords_path)\n",
    "            # Drop non-Hindi translations\n",
    "            columns_to_drop = [\n",
    "                'FrenchKey.txt', 'SpanishKey.txt', 'KurdishKey.txt',\n",
    "                'BengaliKey.txt', 'MandarinKey.txt', 'GreekKey.txt'\n",
    "            ]\n",
    "            df.drop(columns=columns_to_drop, inplace=True)\n",
    "            df.dropna(inplace=True)\n",
    "            return {row['EnglishKey.txt']: row['HindiKey.txt'] for _, row in df.iterrows()}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading keywords: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _add_special_cases(self) -> None:\n",
    "        \"\"\"Add special case translations\"\"\"\n",
    "        special_cases = {\n",
    "            'i': 'ई',\n",
    "            'j': 'जे',\n",
    "            'k': 'के'\n",
    "        }\n",
    "        self.keywords.update(special_cases)\n",
    "\n",
    "    def get_translation(self, word: str) -> Optional[str]:\n",
    "        \"\"\"Get translation for a keyword if available\"\"\"\n",
    "        return self.keywords.get(word)\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    \"\"\"Dataset for code translation\"\"\"\n",
    "    def __init__(self, codes: List[str], indices: List[int]):\n",
    "        self.codes = codes\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.codes)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, any]:\n",
    "        return {\n",
    "            'index': self.indices[idx],\n",
    "            'code': self.codes[idx]\n",
    "        }\n",
    "\n",
    "def custom_collate(batch: List[Dict]) -> Dict[str, List]:\n",
    "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "    return {\n",
    "        'indices': [item['index'] for item in batch],\n",
    "        'codes': [item['code'] for item in batch]\n",
    "    }\n",
    "\n",
    "class CodeTranslator:\n",
    "    def __init__(self, config: Config, keyword_manager: KeywordManager):\n",
    "        self.config = config\n",
    "        self.keyword_manager = keyword_manager\n",
    "        self.translator = GoogleTranslator(\n",
    "            source=config.source_lang,\n",
    "            target=config.target_lang\n",
    "        )\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def process_compound_word(self, word: str) -> str:\n",
    "        \"\"\"Handle translation of compound words with underscores\"\"\"\n",
    "        if '_' in word:\n",
    "            parts = word.split('_')\n",
    "            translated_parts = []\n",
    "            for part in parts:\n",
    "                translated = self.safe_translate(part)\n",
    "                # If translation contains space, replace with underscore\n",
    "                translated = translated.replace(' ', '_') if translated else part\n",
    "                translated_parts.append(translated)\n",
    "            return '_'.join(translated_parts)\n",
    "        return word\n",
    "\n",
    "    def translate_token(self, token: str) -> str:\n",
    "        if token.isspace():\n",
    "            return token\n",
    "        elif '_' in token:\n",
    "            parts = token.split('_')\n",
    "            translated_parts = []\n",
    "            for part in parts:\n",
    "                if part:\n",
    "                    keyword_trans = self.keyword_manager.get_translation(part)\n",
    "                    if keyword_trans:\n",
    "                        translated_parts.append(keyword_trans)\n",
    "                    else:\n",
    "                        trans = self.safe_translate(part)\n",
    "                        if ' ' in trans:\n",
    "                            trans = trans.replace(' ', '_')\n",
    "                        translated_parts.append(trans)\n",
    "            return '_'.join(translated_parts)\n",
    "        elif token.isalpha():\n",
    "            keyword_trans = self.keyword_manager.get_translation(token)\n",
    "            if keyword_trans:\n",
    "                return keyword_trans.replace(' ', '_')\n",
    "            translation = self.safe_translate(token)\n",
    "            return translation.replace(' ', '_')\n",
    "        return token\n",
    "\n",
    "    def safe_translate(self, text: str) -> str:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                translated = self.translator.translate(text)\n",
    "                if ' ' in translated:\n",
    "                    translated = translated.replace(' ', '_')\n",
    "                if any(c.isascii() and c.isalpha() for c in translated):\n",
    "                    translated = self.translator.translate(text.lower()).replace(' ', '_')\n",
    "                return translated\n",
    "            except Exception as e:\n",
    "                if attempt == self.config.max_retries - 1:\n",
    "                    return text\n",
    "        return text\n",
    "\n",
    "    def translate_line(self, line: str) -> str:\n",
    "        indent = len(line) - len(line.lstrip())\n",
    "        line = line.lstrip()\n",
    "\n",
    "        if not line:\n",
    "            return line\n",
    "\n",
    "        try:\n",
    "            if '#' in line:\n",
    "                code_part, comment_part = line.split('#', 1)\n",
    "                translated_comment = self.safe_translate(comment_part.strip())\n",
    "\n",
    "                if code_part:\n",
    "                    tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code_part)\n",
    "                    translated_tokens = [self.translate_token(token) for token in tokens]\n",
    "                    translated_code = ''.join(translated_tokens)\n",
    "                    return ' ' * indent + translated_code.rstrip() + ' #' + translated_comment\n",
    "                return ' ' * indent + '#' + translated_comment\n",
    "\n",
    "            tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', line)\n",
    "            translated_tokens = [self.translate_token(token) for token in tokens]\n",
    "            return ' ' * indent + ''.join(translated_tokens)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Line translation error: {str(e)}\")\n",
    "            return line\n",
    "    def translate_code(self, code: str) -> str:\n",
    "        if not isinstance(code, str):\n",
    "            return \"\"\n",
    "\n",
    "        if '\\\\n' in code:\n",
    "            lines = code.strip(\"'\\\"\").split('\\\\n')\n",
    "            translated_lines = [self.translate_line(line.strip()) for line in lines]\n",
    "            return '\\\\n '.join(translated_lines)\n",
    "\n",
    "        lines = code.split('\\n')\n",
    "        translated_lines = [self.translate_line(line) for line in lines]\n",
    "        return '\\n'.join(translated_lines)\n",
    "\n",
    "    def process_batch(self, batch: Dict[str, List]) -> Tuple[List[int], List[str]]:\n",
    "        \"\"\"Process a batch of code samples\"\"\"\n",
    "        indices = batch['indices']\n",
    "        codes = batch['codes']\n",
    "\n",
    "        translated_batch = []\n",
    "        for code in codes:\n",
    "            if isinstance(code, torch.Tensor):\n",
    "                code = code.cpu().numpy().item()\n",
    "            translated_code = self.translate_code(code)\n",
    "            translated_batch.append(translated_code)\n",
    "\n",
    "        return indices, translated_batch\n",
    "\n",
    "class TranslationManager:\n",
    "    \"\"\"Manages the overall translation process\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.checkpoint_manager = CheckpointManager(config.checkpoint_path)\n",
    "        self.keyword_manager = KeywordManager(config.keywords_path)\n",
    "        self.translator = CodeTranslator(config, self.keyword_manager)\n",
    "\n",
    "    def prepare_data(self) -> Tuple[pd.DataFrame, List[int]]:\n",
    "        if os.path.exists(self.config.output_path):\n",
    "            results_df = pd.read_csv(self.config.output_path)\n",
    "            input_df = pd.read_csv(self.config.input_path)\n",
    "        else:\n",
    "            input_df = pd.read_csv(self.config.input_path)\n",
    "            if self.config.max_rows:\n",
    "                input_df = input_df.head(self.config.max_rows)\n",
    "            results_df = pd.DataFrame({\n",
    "                'English_code': input_df['English_code'],\n",
    "                'Hindi_code': [None] * len(input_df)\n",
    "            })\n",
    "\n",
    "        unprocessed_indices = self.checkpoint_manager.get_unprocessed_indices(len(input_df))\n",
    "        if self.config.max_rows:\n",
    "            unprocessed_indices = unprocessed_indices[:self.config.max_rows]\n",
    "        return results_df, unprocessed_indices\n",
    "\n",
    "    def process_translations(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Process all translations with checkpointing\"\"\"\n",
    "        try:\n",
    "            results_df, unprocessed_indices = self.prepare_data()\n",
    "\n",
    "            if not unprocessed_indices:\n",
    "                print(\"All items have been processed!\")\n",
    "                return results_df\n",
    "\n",
    "            print(f\"Found {len(unprocessed_indices)} unprocessed items\")\n",
    "\n",
    "            # Create dataset and dataloader\n",
    "            unprocessed_codes = [\n",
    "                results_df.iloc[i]['English_code'] for i in unprocessed_indices\n",
    "            ]\n",
    "            dataset = CodeDataset(unprocessed_codes, unprocessed_indices)\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=custom_collate\n",
    "            )\n",
    "\n",
    "            # Process batches\n",
    "            try:\n",
    "                with tqdm(total=len(unprocessed_indices), desc=\"Translating code\") as pbar:\n",
    "                    for batch in dataloader:\n",
    "                        indices, translated_codes = self.translator.process_batch(batch)\n",
    "\n",
    "                        # Update results and save progress\n",
    "                        for idx, translated_code in zip(indices, translated_codes):\n",
    "                            results_df.at[idx, 'Hindi_code'] = translated_code\n",
    "                            self.checkpoint_manager.mark_processed(idx)\n",
    "\n",
    "                        # Save intermediate results\n",
    "                        results_df.to_csv(self.config.output_path, index=False)\n",
    "                        pbar.update(len(indices))\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nProcess interrupted by user. Saving progress...\")\n",
    "                results_df.to_csv(self.config.output_path, index=False)\n",
    "                return results_df\n",
    "\n",
    "            print(f\"\\nProcessing completed! Results saved to: {self.config.output_path}\")\n",
    "            return results_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during processing: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            if 'results_df' in locals():\n",
    "                results_df.to_csv(self.config.output_path, index=False)\n",
    "                return results_df\n",
    "            return None\n",
    "\n",
    "\n",
    "\"\"\"Main entry point\"\"\"\n",
    "# Create config\n",
    "config = Config(max_rows=50)\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "# Clean up GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize and run translation manager\n",
    "manager = TranslationManager(config)\n",
    "processed_df = manager.process_translations()\n",
    "\n",
    "if processed_df is not None:\n",
    "    print(\"\\nProcessing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, max_rows=None, example_count = 5):\n",
    "        self.input_path = 'data/google_code_translations.csv'\n",
    "        self.output_path = 'data/translations_gpt.csv'\n",
    "        self.checkpoint_path = 'data/checkpoint_gpt.json'\n",
    "        self.max_rows = max_rows\n",
    "        self.batch_size = 5\n",
    "        self.openai_api_key = API_key\n",
    "        self.examples_count = example_count\n",
    "        self.client = OpenAI(api_key=self.openai_api_key)\n",
    "\n",
    "class KeywordReplacer:\n",
    "    def __init__(self):\n",
    "        self.keywords = self._load_keywords()\n",
    "        \n",
    "    def _load_keywords(self):\n",
    "        df = pd.read_csv('data/segregated_data.csv')\n",
    "        columns_to_drop = [\n",
    "            'FrenchKey.txt', 'SpanishKey.txt', 'KurdishKey.txt',\n",
    "            'BengaliKey.txt', 'MandarinKey.txt', 'GreekKey.txt'\n",
    "        ]\n",
    "        df.drop(columns=columns_to_drop, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        return {row['EnglishKey.txt']: row['HindiKey.txt'] for _, row in df.iterrows()}\n",
    "    \n",
    "    def replace_keywords(self, code):\n",
    "        # Split code into tokens while preserving structure\n",
    "        tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code)\n",
    "        translated_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.keywords:\n",
    "                translated_tokens.append(self.keywords[token])\n",
    "            elif token == 'True':\n",
    "                translated_tokens.append('सत्य')\n",
    "            elif token == 'False':\n",
    "                translated_tokens.append('असत्य')\n",
    "            else:\n",
    "                translated_tokens.append(token)\n",
    "                \n",
    "        return ''.join(translated_tokens)\n",
    "\n",
    "class GPTTranslator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.examples = self.load_examples()\n",
    "        self.keyword_replacer = KeywordReplacer()\n",
    "\n",
    "    def load_examples(self):\n",
    "        df = pd.read_csv(self.config.input_path)\n",
    "        return df.head(self.config.examples_count)[['English_code', 'Hindi_code']]\n",
    "\n",
    "    def create_prompt(self, code_to_translate):\n",
    "        examples_text = \"\"\n",
    "        for i, row in self.examples.iterrows():\n",
    "            examples_text += f\"\\n\\nExample {i+1}:\\n\"\n",
    "            examples_text += f\"English code:\\n{row['English_code']}\\n\"\n",
    "            examples_text += f\"Hindi translated code:\\n{row['Hindi_code']}\\n------------------------\\n\"\n",
    "            \n",
    "        prompt = f\"\"\"Complete the translation of this partially English Python code to completely Hindi python code:\n",
    "        - Translate variable names, function names, strings and comments to Hindi\n",
    "        - Join multi-word Hindi translations with underscores\n",
    "        - Break down compound English words separated by underscores and translate each part into sensible Hindi and join them back with underscores\n",
    "        - Preserve code structure and syntax\n",
    "        - Here are some examples of translations:\n",
    "    \n",
    "        {examples_text}\n",
    "        \n",
    "        Now translate partially translated code to completely in Hindi:\n",
    "        {code_to_translate}\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def translate_code(self, code):\n",
    "        # First replace known keywords\n",
    "        partially_translated = self.keyword_replacer.replace_keywords(code)\n",
    "\n",
    "        # Then use GPT to complete the translation\n",
    "        prompt = self.create_prompt(partially_translated)\n",
    "        try:\n",
    "            # print(f\"Prompt:\\n{prompt}\")\n",
    "            response = self.config.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a Expert Python code translator who understands the nuanses of language in coding and converts code from English to  Hindi code while preserving functionality. Return only the translated code without any explanation.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            translated_code = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean up the response to extract just the code\n",
    "            if \"```python\" in translated_code:\n",
    "                translated_code = translated_code.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "            elif \"```\" in translated_code:\n",
    "                translated_code = translated_code.split(\"```\")[1].strip()\n",
    "                \n",
    "            return translated_code\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {str(e)}\")\n",
    "            return code\n",
    "\n",
    "def run_translation(max_rows=None):\n",
    "    config = Config(max_rows=max_rows, example_count=5)\n",
    "    translator = GPTTranslator(config)\n",
    "    \n",
    "    df = pd.read_csv(config.input_path)\n",
    "    if max_rows:\n",
    "        df = df.iloc[config.examples_count:max_rows+config.examples_count]\n",
    "    \n",
    "    results = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        translated = translator.translate_code(row['English_code'])\n",
    "        results.append({\n",
    "            'English_code': row['English_code'],\n",
    "            'Hindi_code': translated\n",
    "        })\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(config.output_path, index=False)\n",
    "    return results_df\n",
    "\n",
    "# Usage\n",
    "translated_df = run_translation(max_rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankitkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Evaluating translations: 5it [00:12,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "def keyword_reverse_translation(code, reverse_keywords):\n",
    "    tokens = re.findall(r'[\\u0900-\\u097F_]+|[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code)\n",
    "    translated_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in reverse_keywords:\n",
    "            translated_tokens.append(reverse_keywords[token])\n",
    "        else:\n",
    "            translated_tokens.append(token)\n",
    "    \n",
    "    return ''.join(translated_tokens)\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.keyword_replacer = KeywordReplacer()\n",
    "        nltk.download('punkt')\n",
    "        self.reverse_keywords = {v: k for k, v in self.keyword_replacer.keywords.items()}\n",
    "\n",
    "    def reverse_translate_code(self, hindi_code):\n",
    "        # First replace known keywords\n",
    "        partially_translated = keyword_reverse_translation(hindi_code, self.reverse_keywords)\n",
    "        \n",
    "        prompt = f\"\"\"Complete the translation of this partially translated Python code to English:\n",
    "        - The code already has Python keywords translated to English\n",
    "        - Translate remaining variable names and comments\n",
    "        - Convert Hindi compound words (with underscores) to appropriate English terms\n",
    "        - Preserve code structure and syntax\n",
    "        \n",
    "        Partially translated code:\n",
    "        {partially_translated}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.config.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a Python code translator converting Hindi code to English.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            return self.clean_code(response.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Reverse translation error: {str(e)}\")\n",
    "            return hindi_code\n",
    "    def clean_code(self, code):\n",
    "        \"\"\"Remove markdown and normalize code\"\"\"\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in code:\n",
    "            code = code.split(\"```\")[1].strip()\n",
    "        return code.strip()\n",
    "\n",
    "    def calculate_bleu(self, original, translated):\n",
    "        \"\"\"Calculate BLEU score between original and translated code\"\"\"\n",
    "        smooth = SmoothingFunction().method1\n",
    "        \n",
    "        # Tokenize the code\n",
    "        def tokenize(code):\n",
    "            return nltk.word_tokenize(code)\n",
    "        \n",
    "        reference = [tokenize(original)]\n",
    "        candidate = tokenize(translated)\n",
    "        \n",
    "        return sentence_bleu(reference, candidate, smoothing_function=smooth)\n",
    "\n",
    "    def evaluate_translations(self, df):\n",
    "        \"\"\"Evaluate translations using round-trip and BLEU score\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), desc=\"Evaluating translations\"):\n",
    "            original = row['English_code']\n",
    "            hindi = row['Hindi_code']\n",
    "            \n",
    "            # Round-trip translation\n",
    "            back_translated = self.reverse_translate_code(hindi)\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu_score = self.calculate_bleu(original, back_translated)\n",
    "            \n",
    "            results.append({\n",
    "                'original': original,\n",
    "                'hindi': hindi,\n",
    "                'back_translated': back_translated,\n",
    "                'bleu_score': bleu_score\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Usage example\n",
    "def run_evaluation():\n",
    "    config = Config()\n",
    "    evaluator = TranslationEvaluator(config)\n",
    "    \n",
    "    # Load translations\n",
    "    df = pd.read_csv('data/translations_gpt.csv')\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_translations(df)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv('data/evaluation_results.csv', index=False)\n",
    "    \n",
    "    # Print average BLEU score\n",
    "    avg_bleu = results['bleu_score'].mean()\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keywords():\n",
    "    keywords = pd.read_csv('data/segregated_data.csv')\n",
    "    keywords.drop(columns=['FrenchKey.txt','SpanishKey.txt','KurdishKey.txt','BengaliKey.txt','MandarinKey.txt','GreekKey.txt'], inplace=True)\n",
    "    keywords.dropna(inplace=True)\n",
    "    keywords_dict = {row['EnglishKey.txt']: row['HindiKey.txt'] for _, row in keywords.iterrows()}\n",
    "    return keywords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = load_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abs': 'निरपेक्ष_मान',\n",
       " 'all': 'सब',\n",
       " 'any': 'कोई',\n",
       " 'ascii': 'आस्की',\n",
       " 'bin': 'द्वि',\n",
       " 'bool': 'बूल',\n",
       " 'bytearray': 'बाइटरे',\n",
       " 'bytes': 'बाइट्स',\n",
       " 'callable': 'बुलाने_योग्य',\n",
       " 'chr': 'अक्षर',\n",
       " 'classmethod': 'वर्ग_विधि',\n",
       " 'compile': 'संकलन',\n",
       " 'complex': 'समिश्र',\n",
       " 'delattr': 'गुणमिट',\n",
       " 'dict': 'कोश',\n",
       " 'dir': 'निर्देशिका',\n",
       " 'divmod': 'भाग_मापांक',\n",
       " 'enumerate': 'गणना',\n",
       " 'eval': 'आंकें',\n",
       " 'exec': 'अमल',\n",
       " 'filter': 'छानें',\n",
       " 'float': 'दश',\n",
       " 'format': 'प्रारूप',\n",
       " 'frozenset': 'जमासेट',\n",
       " 'getattr': 'गुणप्राप्त',\n",
       " 'globals': 'वैश्विक',\n",
       " 'hasattr': 'गुणहै',\n",
       " 'hash': 'हैश',\n",
       " 'help': 'मदद',\n",
       " 'hex': 'षोडश',\n",
       " 'id': 'पहचान',\n",
       " 'input': 'इनपुट',\n",
       " 'int': 'अंक',\n",
       " 'isinstance': 'उदाहरण_है',\n",
       " 'issubclass': 'उपवर्ग_है',\n",
       " 'iter': 'दोहरा',\n",
       " 'len': 'लंबाई',\n",
       " 'list': 'सूची',\n",
       " 'locals': 'लोकल',\n",
       " 'map': 'नक्शा',\n",
       " 'max': 'अधिकतम',\n",
       " 'memoryview': 'स्मृति_दर्शन',\n",
       " 'min': 'न्यूनतम',\n",
       " 'next': 'अगला',\n",
       " 'object': 'चीज़',\n",
       " 'oct': 'अष्ट',\n",
       " 'open': 'खोल',\n",
       " 'ord': 'क्रम',\n",
       " 'pow': 'घात',\n",
       " 'print': 'छापिये ',\n",
       " 'property': 'संपत्ति',\n",
       " 'range': 'रेंज',\n",
       " 'repr': 'लघूरूप',\n",
       " 'reversed': 'उल्टा',\n",
       " 'round': 'गोल',\n",
       " 'set': 'सेट',\n",
       " 'setattr': 'सेटगुण',\n",
       " 'slice': 'फाँक',\n",
       " 'sorted': 'क्रमबद्ध',\n",
       " 'staticmethod': 'स्थिर्विधि',\n",
       " 'str': 'स्ट्रिंग',\n",
       " 'sum': 'योग',\n",
       " 'super': 'सुपरसेट',\n",
       " 'tuple': 'ट्यूपल',\n",
       " 'type': 'प्रकार',\n",
       " 'vars': 'चर',\n",
       " 'zip': 'ज़िप',\n",
       " 'capitalize': 'बड़ा_करें',\n",
       " 'casefold': 'केस_तह',\n",
       " 'center': 'केंद्र',\n",
       " 'count': 'गिनें',\n",
       " 'encode': 'एन्कोड',\n",
       " 'endswith': 'से_ख़त्म',\n",
       " 'expandtabs': 'टैबफैल',\n",
       " 'find': 'ढूँढो',\n",
       " 'format_map': 'प्रारूप_नक्शा',\n",
       " 'index': 'सूचक',\n",
       " 'isalnum': 'अंकाक्षर_है',\n",
       " 'isalpha': 'अक्षर_है',\n",
       " 'isascii': 'आस्की_है',\n",
       " 'isdecimal': 'दशमलव_है',\n",
       " 'isdigit': 'अंक_है',\n",
       " 'isidentifier': 'पहचान_है',\n",
       " 'islower': 'कम_है',\n",
       " 'isnumeric': 'नंबर_है',\n",
       " 'isprintable': 'प्रिंट_योग्य',\n",
       " 'isspace': 'ख़ाली_है',\n",
       " 'istitle': 'शीर्षक_है',\n",
       " 'isupper': 'बड़ा_है',\n",
       " 'join': 'जोड़',\n",
       " 'ljust': 'बाएँ_संरेखित',\n",
       " 'lower': 'छोटा',\n",
       " 'lstrip': 'बाईंहट',\n",
       " 'maketrans': 'अनुवाद',\n",
       " 'partition': 'विभाजन',\n",
       " 'replace': 'बदल',\n",
       " 'rfind': 'उलटखोज',\n",
       " 'rindex': 'उलटसूचक',\n",
       " 'rjust': 'दाएँ_संरेखित',\n",
       " 'rpartition': 'दाईं_ओर_से_विभाजन',\n",
       " 'rsplit': 'दाएँ_बाँट',\n",
       " 'rstrip': 'दाएँ_उघाड़',\n",
       " 'split': 'बाँट',\n",
       " 'splitlines': 'पंक्ति_विभाजन',\n",
       " 'startswith': 'से_शुरू',\n",
       " 'strip': 'उघाड़',\n",
       " 'swapcase': 'केस_बदल',\n",
       " 'title': 'शीर्षक',\n",
       " 'translate': 'अनुवाद',\n",
       " 'upper': 'बड़ा',\n",
       " 'zfill': 'शून्य_भर',\n",
       " 'append': 'संलग्न',\n",
       " 'clear': 'स्पष्ट',\n",
       " 'copy': 'कॉपी',\n",
       " 'extend': 'बढ़ा',\n",
       " 'insert': 'डाल',\n",
       " 'pop': 'पॉप',\n",
       " 'remove': 'निकाल',\n",
       " 'reverse': 'उलट',\n",
       " 'sort': 'क्रमित',\n",
       " 'fromkeys': 'कीज़_से',\n",
       " 'get': 'पाना',\n",
       " 'items': 'सामान',\n",
       " 'keys': 'कीज़',\n",
       " 'popitem': 'पॉप_आइटम',\n",
       " 'setdefault': 'सेट_डिफ़ॉल्ट',\n",
       " 'update': 'उद्यतन',\n",
       " 'values': 'मान',\n",
       " 'add': 'योग',\n",
       " 'difference': 'अंतर',\n",
       " 'difference_update': 'अंतर_अद्यतन',\n",
       " 'discard': 'खारिज',\n",
       " 'intersection': 'प्रतिच्छेद',\n",
       " 'intersection_update': 'प्रतिच्छेद_उद्यतन',\n",
       " 'isdisjoint': 'असंयुक्त_है',\n",
       " 'issubset': 'सबसेट_है',\n",
       " 'issuperset': 'सुपरसेट_है',\n",
       " 'symmetric_difference': 'सममित_अंतर',\n",
       " 'symmetric_difference_update': 'सममित_अंतर_अद्यतन',\n",
       " 'union': 'संघ',\n",
       " 'close': 'बंद',\n",
       " 'detach': 'जुदा',\n",
       " 'fileno': 'फ़ाइलनं',\n",
       " 'flush': 'फ़्लश',\n",
       " 'isatty': 'संवादात्मक_है',\n",
       " 'read': 'पढ़',\n",
       " 'readable': 'पढ़ने_योग्य',\n",
       " 'readline': 'पंक्ति_पढ़',\n",
       " 'readlines': 'पंक्तियाँ_पढ़ें',\n",
       " 'seek': 'माँग',\n",
       " 'seekable': 'खोज_योग्य',\n",
       " 'tell': 'बताना',\n",
       " 'truncate': 'काट-छांट',\n",
       " 'writable': 'लिखने_योग्य',\n",
       " 'write': 'लिखो',\n",
       " 'writelines': 'पंक्तियाँ_लिखो',\n",
       " 'and': 'और',\n",
       " 'as': 'की_तरह',\n",
       " 'assert': 'दावा',\n",
       " 'break': 'तोड़',\n",
       " 'class': 'वर्ग',\n",
       " 'continue': 'जारी',\n",
       " 'def': 'परिभाषा',\n",
       " 'del': 'मिट',\n",
       " 'elif': 'वरना_यदि',\n",
       " 'else': 'वरना',\n",
       " 'except': 'सिवाय',\n",
       " 'False ': 'असत्य',\n",
       " 'finally': 'अंत_में',\n",
       " 'for': 'के_लिए',\n",
       " 'from': 'से',\n",
       " 'global': 'वैश्विक',\n",
       " 'if': 'यदि',\n",
       " 'import': 'आयात',\n",
       " 'in': 'में',\n",
       " 'is': 'है',\n",
       " 'lambda': 'लैम्ब्डा',\n",
       " 'nonlocal': 'गैरलोकल',\n",
       " 'not': 'ना',\n",
       " 'or ': 'या',\n",
       " 'pass': 'जाने_दो',\n",
       " 'raise': 'उठाओ',\n",
       " 'return': 'वापस',\n",
       " 'True ': 'सत्य',\n",
       " 'try': 'प्रयत्न',\n",
       " 'while': 'जबतक',\n",
       " 'with': 'साथ',\n",
       " 'yield': 'त्याग',\n",
       " 'ArithmeticError': 'अंकगणितीय_त्रुटि',\n",
       " 'AssertionError': 'दावा_त्रुटि',\n",
       " 'AttributeError': 'गुण_त्रुटि',\n",
       " 'Exception': 'अपवाद',\n",
       " 'EOFError': 'फ़ाइल_अंत_त्रुटि',\n",
       " 'FloatingPointError': 'दशामवलव_त्रुटि',\n",
       " 'GeneratorExit': 'जेनरेटर_निकास',\n",
       " 'ImportError': 'आयात_त्रुटि',\n",
       " 'IndentationError ': 'खाँचा_त्रुटि',\n",
       " 'IndexError': 'सूचकांक_त्रुटि',\n",
       " 'KeyError': 'की_त्रुटि',\n",
       " 'KeyboardInterrupt': 'कीबोर्ड_टोक',\n",
       " 'LookupError': 'खोज_त्रुटि',\n",
       " 'MemoryError': 'स्मृति_त्रुटि',\n",
       " 'NameError': 'नाम_त्रुटि',\n",
       " 'NotImplementedError': 'अकार्यान्वित_त्रुटि',\n",
       " 'OSError': 'ओएस_त्रुटि',\n",
       " 'OverflowError': 'अतिप्रवाह_त्रुटि',\n",
       " 'ReferenceError': 'संदर्भ_त्रुटि',\n",
       " 'RuntimeError': 'रनटाइम_त्रुटि',\n",
       " 'StopIteration': 'दोहराना_रोकें',\n",
       " 'SyntaxError': 'वाक्यविन्यास_त्रुटि',\n",
       " 'TabError': 'टैब_त्रुटि',\n",
       " 'SystemError': 'सिस्टम_त्रुटि',\n",
       " 'SystemExit': 'सिस्टम_निकास',\n",
       " 'TypeError': 'प्रकार_त्रुटि',\n",
       " 'UnboundLocalError': 'असीम_लोकल_त्रुटि',\n",
       " 'UnicodeError': 'यूनिकोड_त्रुटि',\n",
       " 'UnicodeEncodeError': 'यूनिकोड_एनकोड_त्रुटि',\n",
       " 'UnicodeDecodeError': 'यूनिकोड_डिकोड_त्रुटि',\n",
       " 'UnicodeTranslateError': 'यूनिकोड_अनुवाद_त्रुटि',\n",
       " 'ValueError': 'मान_त्रुटि',\n",
       " 'ZeroDivisionError': 'शून्य_भाग_त्रुटि'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for code translation\"\"\"\n",
    "    def __init__(self, codes):\n",
    "        self.codes = codes\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.codes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.codes[idx]\n",
    "\n",
    "class GPUCodeTranslator:\n",
    "    def __init__(self, keywords_dict, batch_size=32, device=None):\n",
    "        self.translator = GoogleTranslator(source='en', target='hi')\n",
    "        self.keywords = keywords_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Add common translations\n",
    "        self.special_translations = {\n",
    "            'i': 'ई',\n",
    "            'j': 'जे',\n",
    "            'k': 'के'\n",
    "        }\n",
    "        self.keywords.update(self.special_translations)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "    \n",
    "    def safe_translate(self, text, max_retries=3):\n",
    "        \"\"\"Translate text with retry mechanism\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return text\n",
    "            \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(0.2)  # Reduced delay for GPU processing\n",
    "                return self.translator.translate(text)\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    return text\n",
    "                time.sleep(0.5)\n",
    "        return text\n",
    "    \n",
    "    def translate_line(self, line):\n",
    "        \"\"\"Translate a single line of code\"\"\"\n",
    "        # print(f\"\\nTranslating line: {line}\")  # Debug print\n",
    "        indent = len(line) - len(line.lstrip())\n",
    "        line = line.lstrip()\n",
    "        \n",
    "        if not line:\n",
    "            return line\n",
    "        \n",
    "        try:\n",
    "            # Handle comments\n",
    "            if '#' in line:\n",
    "                code_part, comment_part = line.split('#', 1)\n",
    "                translated_comment = self.safe_translate(comment_part.strip())\n",
    "                # print(f\"Translated comment: {translated_comment}\")  # Debug print\n",
    "                \n",
    "                if code_part:\n",
    "                    tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code_part)\n",
    "                    translated_tokens = []\n",
    "                    \n",
    "                    for token in tokens:\n",
    "                        if token.isspace():\n",
    "                            translated_tokens.append(token)\n",
    "                        elif token.isalpha():\n",
    "                            translated = self.keywords.get(token, self.safe_translate(token))\n",
    "                            # print(f\"Token '{token}' translated to '{translated}'\")  # Debug print\n",
    "                            translated_tokens.append(translated)\n",
    "                        else:\n",
    "                            translated_tokens.append(token)\n",
    "                    \n",
    "                    translated_code = ''.join(translated_tokens)\n",
    "                    return ' ' * indent + translated_code.rstrip() + ' #' + translated_comment\n",
    "                else:\n",
    "                    return ' ' * indent + '#' + translated_comment\n",
    "            \n",
    "            # Handle code-only lines\n",
    "            tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', line)\n",
    "            translated_tokens = []\n",
    "            \n",
    "            for token in tokens:\n",
    "                if token.isspace():\n",
    "                    translated_tokens.append(token)\n",
    "                elif token.isalpha():\n",
    "                    translated = self.keywords.get(token, self.safe_translate(token))\n",
    "                    # print(f\"Token '{token}' translated to '{translated}'\")  # Debug print\n",
    "                    translated_tokens.append(translated)\n",
    "                else:\n",
    "                    translated_tokens.append(token)\n",
    "            \n",
    "            return ' ' * indent + ''.join(translated_tokens)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Line translation error: {str(e)}\")\n",
    "            return line\n",
    "    \n",
    "    def translate_code(self, code):\n",
    "        \"\"\"Translate complete code block\"\"\"\n",
    "        if not isinstance(code, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Handle literal \\n in the input\n",
    "        if '\\\\n' in code:\n",
    "            lines = code.strip(\"'\\\"\").split('\\\\n')\n",
    "            translated_lines = []\n",
    "            \n",
    "            for line in lines:\n",
    "                translated_line = self.translate_line(line.strip())\n",
    "                translated_lines.append(translated_line)\n",
    "            \n",
    "            return '\\\\n '.join(translated_lines)\n",
    "        \n",
    "        # Handle regular newlines\n",
    "        lines = code.split('\\n')\n",
    "        translated_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            translated_line = self.translate_line(line)\n",
    "            translated_lines.append(translated_line)\n",
    "        \n",
    "        return '\\n'.join(translated_lines)\n",
    "    \n",
    "    def process_batch(self, batch):\n",
    "        \"\"\"Process a batch of code samples in parallel\"\"\"\n",
    "        translated_batch = []\n",
    "        \n",
    "        # Move batch to GPU if available\n",
    "        if isinstance(batch, torch.Tensor):\n",
    "            batch = batch.to(self.device)\n",
    "        \n",
    "        for code in batch:\n",
    "            if isinstance(code, torch.Tensor):\n",
    "                code = code.cpu().numpy().item()\n",
    "            translated_code = self.translate_code(code)\n",
    "            translated_batch.append(translated_code)\n",
    "        \n",
    "        return translated_batch\n",
    "\n",
    "def process_test_samples(input_path, output_path, keywords_dict, num_samples=10):\n",
    "    \"\"\"Process just a few samples as a test\"\"\"\n",
    "    # Initialize translator\n",
    "    translator = GPUCodeTranslator(keywords_dict, batch_size=5)  # Smaller batch size for testing\n",
    "    \n",
    "    try:\n",
    "        # Load dataset and take first few samples\n",
    "        df = pd.read_csv(input_path)\n",
    "        test_df = df.head(num_samples)\n",
    "        # print(f\"Processing {num_samples} test samples\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = CodeDataset(test_df['English_code'].tolist())\n",
    "        dataloader = DataLoader(dataset, batch_size=5, shuffle=False)\n",
    "        \n",
    "        # Process batches\n",
    "        translated_codes = []\n",
    "        for batch in tqdm(dataloader, desc=\"Translating test samples\"):\n",
    "            translated_batch = translator.process_batch(batch)\n",
    "            translated_codes.extend(translated_batch)\n",
    "        \n",
    "        # Create results DataFrame\n",
    "        results_df = pd.DataFrame({\n",
    "            'English_code': test_df['English_code'],\n",
    "            'Hindi_code': translated_codes\n",
    "        })\n",
    "        \n",
    "        # Save results\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nTest results saved to: {output_path}\")\n",
    "        \n",
    "        # Display all translations\n",
    "        print(\"\\nAll Translations:\")\n",
    "        print(\"-\" * 80)\n",
    "        for idx, row in results_df.iterrows():\n",
    "            print(f\"\\nSample {idx + 1}:\")\n",
    "            print(\"Original:\")\n",
    "            print(row['English_code'])\n",
    "            print(\"\\nTranslated:\")\n",
    "            print(row['Hindi_code'])\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        return results_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing test samples: {str(e)}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating test samples:  30%|███       | 6/20 [05:57<14:08, 60.60s/it]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set up GPU memory management\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # Load keywords\n",
    "    keywords = load_keywords()\n",
    "    \n",
    "    # Set paths\n",
    "    input_path = 'data/extracted_code_blocks.csv'\n",
    "    output_path = 'data/code_translations.csv'\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # Process test samples\n",
    "    processed_df = process_test_samples(input_path, output_path, keywords, num_samples=100)\n",
    "    \n",
    "    if processed_df is not None:\n",
    "        print(\"\\nTest run completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
