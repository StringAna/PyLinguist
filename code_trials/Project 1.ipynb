{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import traceback\n",
    "import warnings\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from deep_translator import GoogleTranslator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder with the name of data\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nfor i in range(10):  # First digit\\n    for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\ndef count_distinct_states(matrix):\\n    coun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\ndef remove_spaces_and_punctuation(s):\\n    r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nimport math\\n\\ndef is_prime(n):\\n    # Check...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nclass String:\\n    def __init__(self, string...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        English_code\n",
       "0  \\nfor i in range(10):  # First digit\\n    for ...\n",
       "1  \\ndef count_distinct_states(matrix):\\n    coun...\n",
       "2  \\ndef remove_spaces_and_punctuation(s):\\n    r...\n",
       "3  \\nimport math\\n\\ndef is_prime(n):\\n    # Check...\n",
       "4  \\nclass String:\\n    def __init__(self, string..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from Hugging Face Datasets and store the output column after extracting the python code into a csv file \n",
    "ds = load_dataset(\"jtatman/python-code-dataset-500k\")\n",
    "ds = pd.DataFrame(ds['train'])\n",
    "ds = ds.drop(['instruction', 'system'], axis=1)\n",
    "ds['English_code'] = ds['output'].apply(lambda x: re.search(r'```python(.*?)```', x, re.DOTALL).group(1) if re.search(r'```python(.*?)```', x, re.DOTALL) else None)\n",
    "ds = ds.drop(['output'], axis=1)\n",
    "ds.to_csv('data/python_code_dataset.csv', index=False)\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found 50 unprocessed items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating code: 100%|██████████| 50/50 [06:35<00:00,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed! Results saved to: data/google_code_translations.csv\n",
      "\n",
      "Processing completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    def __init__(self, max_rows=None):\n",
    "        self.input_path = 'data/python_code_dataset.csv'\n",
    "        self.output_path = 'data/google_code_translations.csv'\n",
    "        self.checkpoint_path = 'data/translation_checkpoint.json'\n",
    "        self.keywords_path = 'data/segregated_data.csv'\n",
    "        self.batch_size = 5\n",
    "        self.source_lang = 'en'\n",
    "        self.target_lang = 'hi'\n",
    "        self.sleep_time = 0.2\n",
    "        self.max_retries = 3\n",
    "        self.max_rows = max_rows\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Manages saving and loading of translation progress\"\"\"\n",
    "    def __init__(self, checkpoint_path: str):\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.processed_indices = set()\n",
    "        self._load_checkpoint()\n",
    "\n",
    "    def _load_checkpoint(self) -> None:\n",
    "        \"\"\"Load existing checkpoint if available\"\"\"\n",
    "        if os.path.exists(self.checkpoint_path):\n",
    "            try:\n",
    "                with open(self.checkpoint_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                self.processed_indices = set(data.get('processed_indices', []))\n",
    "                print(f\"Loaded checkpoint with {len(self.processed_indices)} processed items\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint: {str(e)}\")\n",
    "                self.processed_indices = set()\n",
    "\n",
    "    def save_checkpoint(self) -> None:\n",
    "        \"\"\"Save current progress to checkpoint file\"\"\"\n",
    "        try:\n",
    "            data = {'processed_indices': list(self.processed_indices)}\n",
    "            with open(self.checkpoint_path, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving checkpoint: {str(e)}\")\n",
    "\n",
    "    def mark_processed(self, index: int) -> None:\n",
    "        \"\"\"Mark an item as processed and save checkpoint\"\"\"\n",
    "        self.processed_indices.add(index)\n",
    "        self.save_checkpoint()\n",
    "\n",
    "    def is_processed(self, index: int) -> bool:\n",
    "        \"\"\"Check if an item has been processed\"\"\"\n",
    "        return index in self.processed_indices\n",
    "\n",
    "    def get_unprocessed_indices(self, total_items: int) -> List[int]:\n",
    "        \"\"\"Get list of indices that haven't been processed yet\"\"\"\n",
    "        return [i for i in range(total_items) if not self.is_processed(i)]\n",
    "\n",
    "class KeywordManager:\n",
    "    \"\"\"Manages programming keyword translations\"\"\"\n",
    "    def __init__(self, keywords_path: str):\n",
    "        self.keywords_path = keywords_path\n",
    "        self.keywords = self._load_keywords()\n",
    "        self._add_special_cases()\n",
    "\n",
    "    def _load_keywords(self) -> Dict[str, str]:\n",
    "        \"\"\"Load keyword translations from file\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.keywords_path)\n",
    "            # Drop non-Hindi translations\n",
    "            columns_to_drop = [\n",
    "                'FrenchKey.txt', 'SpanishKey.txt', 'KurdishKey.txt',\n",
    "                'BengaliKey.txt', 'MandarinKey.txt', 'GreekKey.txt'\n",
    "            ]\n",
    "            df.drop(columns=columns_to_drop, inplace=True)\n",
    "            df.dropna(inplace=True)\n",
    "            return {row['EnglishKey.txt']: row['HindiKey.txt'] for _, row in df.iterrows()}\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading keywords: {str(e)}\")\n",
    "            return {}\n",
    "\n",
    "    def _add_special_cases(self) -> None:\n",
    "        \"\"\"Add special case translations\"\"\"\n",
    "        special_cases = {\n",
    "            'i': 'ई',\n",
    "            'j': 'जे',\n",
    "            'k': 'के'\n",
    "        }\n",
    "        self.keywords.update(special_cases)\n",
    "\n",
    "    def get_translation(self, word: str) -> Optional[str]:\n",
    "        \"\"\"Get translation for a keyword if available\"\"\"\n",
    "        return self.keywords.get(word)\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    \"\"\"Dataset for code translation\"\"\"\n",
    "    def __init__(self, codes: List[str], indices: List[int]):\n",
    "        self.codes = codes\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.codes)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, any]:\n",
    "        return {\n",
    "            'index': self.indices[idx],\n",
    "            'code': self.codes[idx]\n",
    "        }\n",
    "\n",
    "def custom_collate(batch: List[Dict]) -> Dict[str, List]:\n",
    "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "    return {\n",
    "        'indices': [item['index'] for item in batch],\n",
    "        'codes': [item['code'] for item in batch]\n",
    "    }\n",
    "\n",
    "class CodeTranslator:\n",
    "    def __init__(self, config: Config, keyword_manager: KeywordManager):\n",
    "        self.config = config\n",
    "        self.keyword_manager = keyword_manager\n",
    "        self.translator = GoogleTranslator(\n",
    "            source=config.source_lang,\n",
    "            target=config.target_lang\n",
    "        )\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "    def process_compound_word(self, word: str) -> str:\n",
    "        \"\"\"Handle translation of compound words with underscores\"\"\"\n",
    "        if '_' in word:\n",
    "            parts = word.split('_')\n",
    "            translated_parts = []\n",
    "            for part in parts:\n",
    "                translated = self.safe_translate(part)\n",
    "                # If translation contains space, replace with underscore\n",
    "                translated = translated.replace(' ', '_') if translated else part\n",
    "                translated_parts.append(translated)\n",
    "            return '_'.join(translated_parts)\n",
    "        return word\n",
    "\n",
    "    def translate_token(self, token: str) -> str:\n",
    "        if token.isspace():\n",
    "            return token\n",
    "        elif '_' in token:\n",
    "            parts = token.split('_')\n",
    "            translated_parts = []\n",
    "            for part in parts:\n",
    "                if part:\n",
    "                    keyword_trans = self.keyword_manager.get_translation(part)\n",
    "                    if keyword_trans:\n",
    "                        translated_parts.append(keyword_trans)\n",
    "                    else:\n",
    "                        trans = self.safe_translate(part)\n",
    "                        if ' ' in trans:\n",
    "                            trans = trans.replace(' ', '_')\n",
    "                        translated_parts.append(trans)\n",
    "            return '_'.join(translated_parts)\n",
    "        elif token.isalpha():\n",
    "            keyword_trans = self.keyword_manager.get_translation(token)\n",
    "            if keyword_trans:\n",
    "                return keyword_trans.replace(' ', '_')\n",
    "            translation = self.safe_translate(token)\n",
    "            return translation.replace(' ', '_')\n",
    "        return token\n",
    "\n",
    "    def safe_translate(self, text: str) -> str:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return text\n",
    "\n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                translated = self.translator.translate(text)\n",
    "                if ' ' in translated:\n",
    "                    translated = translated.replace(' ', '_')\n",
    "                if any(c.isascii() and c.isalpha() for c in translated):\n",
    "                    translated = self.translator.translate(text.lower()).replace(' ', '_')\n",
    "                return translated\n",
    "            except Exception as e:\n",
    "                if attempt == self.config.max_retries - 1:\n",
    "                    return text\n",
    "        return text\n",
    "\n",
    "    def translate_line(self, line: str) -> str:\n",
    "        indent = len(line) - len(line.lstrip())\n",
    "        line = line.lstrip()\n",
    "\n",
    "        if not line:\n",
    "            return line\n",
    "\n",
    "        try:\n",
    "            if '#' in line:\n",
    "                code_part, comment_part = line.split('#', 1)\n",
    "                translated_comment = self.safe_translate(comment_part.strip())\n",
    "\n",
    "                if code_part:\n",
    "                    tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code_part)\n",
    "                    translated_tokens = [self.translate_token(token) for token in tokens]\n",
    "                    translated_code = ''.join(translated_tokens)\n",
    "                    return ' ' * indent + translated_code.rstrip() + ' #' + translated_comment\n",
    "                return ' ' * indent + '#' + translated_comment\n",
    "\n",
    "            tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', line)\n",
    "            translated_tokens = [self.translate_token(token) for token in tokens]\n",
    "            return ' ' * indent + ''.join(translated_tokens)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Line translation error: {str(e)}\")\n",
    "            return line\n",
    "    def translate_code(self, code: str) -> str:\n",
    "        if not isinstance(code, str):\n",
    "            return \"\"\n",
    "\n",
    "        if '\\\\n' in code:\n",
    "            lines = code.strip(\"'\\\"\").split('\\\\n')\n",
    "            translated_lines = [self.translate_line(line.strip()) for line in lines]\n",
    "            return '\\\\n '.join(translated_lines)\n",
    "\n",
    "        lines = code.split('\\n')\n",
    "        translated_lines = [self.translate_line(line) for line in lines]\n",
    "        return '\\n'.join(translated_lines)\n",
    "\n",
    "    def process_batch(self, batch: Dict[str, List]) -> Tuple[List[int], List[str]]:\n",
    "        \"\"\"Process a batch of code samples\"\"\"\n",
    "        indices = batch['indices']\n",
    "        codes = batch['codes']\n",
    "\n",
    "        translated_batch = []\n",
    "        for code in codes:\n",
    "            if isinstance(code, torch.Tensor):\n",
    "                code = code.cpu().numpy().item()\n",
    "            translated_code = self.translate_code(code)\n",
    "            translated_batch.append(translated_code)\n",
    "\n",
    "        return indices, translated_batch\n",
    "\n",
    "class TranslationManager:\n",
    "    \"\"\"Manages the overall translation process\"\"\"\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.checkpoint_manager = CheckpointManager(config.checkpoint_path)\n",
    "        self.keyword_manager = KeywordManager(config.keywords_path)\n",
    "        self.translator = CodeTranslator(config, self.keyword_manager)\n",
    "\n",
    "    def prepare_data(self) -> Tuple[pd.DataFrame, List[int]]:\n",
    "        if os.path.exists(self.config.output_path):\n",
    "            results_df = pd.read_csv(self.config.output_path)\n",
    "            input_df = pd.read_csv(self.config.input_path)\n",
    "        else:\n",
    "            input_df = pd.read_csv(self.config.input_path)\n",
    "            if self.config.max_rows:\n",
    "                input_df = input_df.head(self.config.max_rows)\n",
    "            results_df = pd.DataFrame({\n",
    "                'English_code': input_df['English_code'],\n",
    "                'Hindi_code': [None] * len(input_df)\n",
    "            })\n",
    "\n",
    "        unprocessed_indices = self.checkpoint_manager.get_unprocessed_indices(len(input_df))\n",
    "        if self.config.max_rows:\n",
    "            unprocessed_indices = unprocessed_indices[:self.config.max_rows]\n",
    "        return results_df, unprocessed_indices\n",
    "\n",
    "    def process_translations(self) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Process all translations with checkpointing\"\"\"\n",
    "        try:\n",
    "            results_df, unprocessed_indices = self.prepare_data()\n",
    "\n",
    "            if not unprocessed_indices:\n",
    "                print(\"All items have been processed!\")\n",
    "                return results_df\n",
    "\n",
    "            print(f\"Found {len(unprocessed_indices)} unprocessed items\")\n",
    "\n",
    "            # Create dataset and dataloader\n",
    "            unprocessed_codes = [\n",
    "                results_df.iloc[i]['English_code'] for i in unprocessed_indices\n",
    "            ]\n",
    "            dataset = CodeDataset(unprocessed_codes, unprocessed_indices)\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                shuffle=False,\n",
    "                collate_fn=custom_collate\n",
    "            )\n",
    "\n",
    "            # Process batches\n",
    "            try:\n",
    "                with tqdm(total=len(unprocessed_indices), desc=\"Translating code\") as pbar:\n",
    "                    for batch in dataloader:\n",
    "                        indices, translated_codes = self.translator.process_batch(batch)\n",
    "\n",
    "                        # Update results and save progress\n",
    "                        for idx, translated_code in zip(indices, translated_codes):\n",
    "                            results_df.at[idx, 'Hindi_code'] = translated_code\n",
    "                            self.checkpoint_manager.mark_processed(idx)\n",
    "\n",
    "                        # Save intermediate results\n",
    "                        results_df.to_csv(self.config.output_path, index=False)\n",
    "                        pbar.update(len(indices))\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nProcess interrupted by user. Saving progress...\")\n",
    "                results_df.to_csv(self.config.output_path, index=False)\n",
    "                return results_df\n",
    "\n",
    "            print(f\"\\nProcessing completed! Results saved to: {self.config.output_path}\")\n",
    "            return results_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during processing: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            if 'results_df' in locals():\n",
    "                results_df.to_csv(self.config.output_path, index=False)\n",
    "                return results_df\n",
    "            return None\n",
    "\n",
    "\n",
    "\"\"\"Main entry point\"\"\"\n",
    "# Create config\n",
    "config = Config(max_rows=50)\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "\n",
    "# Clean up GPU memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize and run translation manager\n",
    "manager = TranslationManager(config)\n",
    "processed_df = manager.process_translations()\n",
    "\n",
    "if processed_df is not None:\n",
    "    print(\"\\nProcessing completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_key = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, max_rows=None, example_count = 5):\n",
    "        self.input_path = 'data/google_code_translations.csv'\n",
    "        self.output_path = 'data/translations_gpt.csv'\n",
    "        self.checkpoint_path = 'data/checkpoint_gpt.json'\n",
    "        self.max_rows = max_rows\n",
    "        self.batch_size = 5\n",
    "        self.openai_api_key = API_key\n",
    "        self.examples_count = example_count\n",
    "        self.client = OpenAI(api_key=self.openai_api_key)\n",
    "\n",
    "class KeywordReplacer:\n",
    "    def __init__(self):\n",
    "        self.keywords = self._load_keywords()\n",
    "        \n",
    "    def _load_keywords(self):\n",
    "        df = pd.read_csv('data/segregated_data.csv')\n",
    "        columns_to_drop = [\n",
    "            'FrenchKey.txt', 'SpanishKey.txt', 'KurdishKey.txt',\n",
    "            'BengaliKey.txt', 'MandarinKey.txt', 'GreekKey.txt'\n",
    "        ]\n",
    "        df.drop(columns=columns_to_drop, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        return {row['EnglishKey.txt']: row['HindiKey.txt'] for _, row in df.iterrows()}\n",
    "    \n",
    "    def replace_keywords(self, code):\n",
    "        # Split code into tokens while preserving structure\n",
    "        tokens = re.findall(r'[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code)\n",
    "        translated_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.keywords:\n",
    "                translated_tokens.append(self.keywords[token])\n",
    "            elif token == 'True':\n",
    "                translated_tokens.append('सत्य')\n",
    "            elif token == 'False':\n",
    "                translated_tokens.append('असत्य')\n",
    "            else:\n",
    "                translated_tokens.append(token)\n",
    "                \n",
    "        return ''.join(translated_tokens)\n",
    "\n",
    "class GPTTranslator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.examples = self.load_examples()\n",
    "        self.keyword_replacer = KeywordReplacer()\n",
    "\n",
    "    def load_examples(self):\n",
    "        df = pd.read_csv(self.config.input_path)\n",
    "        return df.head(self.config.examples_count)[['English_code', 'Hindi_code']]\n",
    "\n",
    "    def create_prompt(self, code_to_translate):\n",
    "        examples_text = \"\"\n",
    "        for i, row in self.examples.iterrows():\n",
    "            examples_text += f\"\\n\\nExample {i+1}:\\n\"\n",
    "            examples_text += f\"English code:\\n{row['English_code']}\\n\"\n",
    "            examples_text += f\"Hindi translated code:\\n{row['Hindi_code']}\\n------------------------\\n\"\n",
    "            \n",
    "        prompt = f\"\"\"Complete the translation of this partially English Python code to completely Hindi python code:\n",
    "        - Translate variable names, function names, strings and comments to Hindi\n",
    "        - Join multi-word Hindi translations with underscores\n",
    "        - Break down compound English words separated by underscores and translate each part into sensible Hindi and join them back with underscores\n",
    "        - Preserve code structure and syntax\n",
    "        - Here are some examples of translations:\n",
    "    \n",
    "        {examples_text}\n",
    "        \n",
    "        Now translate partially translated code to completely in Hindi:\n",
    "        {code_to_translate}\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "\n",
    "    def translate_code(self, code):\n",
    "        # First replace known keywords\n",
    "        partially_translated = self.keyword_replacer.replace_keywords(code)\n",
    "\n",
    "        # Then use GPT to complete the translation\n",
    "        prompt = self.create_prompt(partially_translated)\n",
    "        try:\n",
    "            # print(f\"Prompt:\\n{prompt}\")\n",
    "            response = self.config.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a Expert Python code translator who understands the nuanses of language in coding and converts code from English to  Hindi code while preserving functionality. Return only the translated code without any explanation.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            translated_code = response.choices[0].message.content.strip()\n",
    "\n",
    "            # Clean up the response to extract just the code\n",
    "            if \"```python\" in translated_code:\n",
    "                translated_code = translated_code.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "            elif \"```\" in translated_code:\n",
    "                translated_code = translated_code.split(\"```\")[1].strip()\n",
    "                \n",
    "            return translated_code\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {str(e)}\")\n",
    "            return code\n",
    "\n",
    "def run_translation(max_rows=None):\n",
    "    config = Config(max_rows=max_rows, example_count=5)\n",
    "    translator = GPTTranslator(config)\n",
    "    \n",
    "    df = pd.read_csv(config.input_path)\n",
    "    if max_rows:\n",
    "        df = df.iloc[config.examples_count:max_rows+config.examples_count]\n",
    "    \n",
    "    results = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        translated = translator.translate_code(row['English_code'])\n",
    "        results.append({\n",
    "            'English_code': row['English_code'],\n",
    "            'Hindi_code': translated\n",
    "        })\n",
    "        \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(config.output_path, index=False)\n",
    "    return results_df\n",
    "\n",
    "# Usage\n",
    "translated_df = run_translation(max_rows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ankitkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Evaluating translations: 5it [00:12,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.8150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "def keyword_reverse_translation(code, reverse_keywords):\n",
    "    tokens = re.findall(r'[\\u0900-\\u097F_]+|[a-zA-Z_]+|\\d+|[^\\w\\s]|\\s+', code)\n",
    "    translated_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in reverse_keywords:\n",
    "            translated_tokens.append(reverse_keywords[token])\n",
    "        else:\n",
    "            translated_tokens.append(token)\n",
    "    \n",
    "    return ''.join(translated_tokens)\n",
    "\n",
    "class TranslationEvaluator:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.keyword_replacer = KeywordReplacer()\n",
    "        nltk.download('punkt')\n",
    "        self.reverse_keywords = {v: k for k, v in self.keyword_replacer.keywords.items()}\n",
    "\n",
    "    def reverse_translate_code(self, hindi_code):\n",
    "        # First replace known keywords\n",
    "        partially_translated = keyword_reverse_translation(hindi_code, self.reverse_keywords)\n",
    "        \n",
    "        prompt = f\"\"\"Complete the translation of this partially translated Python code to English:\n",
    "        - The code already has Python keywords translated to English\n",
    "        - Translate remaining variable names and comments\n",
    "        - Convert Hindi compound words (with underscores) to appropriate English terms\n",
    "        - Preserve code structure and syntax\n",
    "        \n",
    "        Partially translated code:\n",
    "        {partially_translated}\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.config.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a Python code translator converting Hindi code to English.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            return self.clean_code(response.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Reverse translation error: {str(e)}\")\n",
    "            return hindi_code\n",
    "    def clean_code(self, code):\n",
    "        \"\"\"Remove markdown and normalize code\"\"\"\n",
    "        if \"```python\" in code:\n",
    "            code = code.split(\"```python\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in code:\n",
    "            code = code.split(\"```\")[1].strip()\n",
    "        return code.strip()\n",
    "\n",
    "    def calculate_bleu(self, original, translated):\n",
    "        \"\"\"Calculate BLEU score between original and translated code\"\"\"\n",
    "        smooth = SmoothingFunction().method1\n",
    "        \n",
    "        # Tokenize the code\n",
    "        def tokenize(code):\n",
    "            return nltk.word_tokenize(code)\n",
    "        \n",
    "        reference = [tokenize(original)]\n",
    "        candidate = tokenize(translated)\n",
    "        \n",
    "        return sentence_bleu(reference, candidate, smoothing_function=smooth)\n",
    "\n",
    "    def evaluate_translations(self, df):\n",
    "        \"\"\"Evaluate translations using round-trip and BLEU score\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for _, row in tqdm(df.iterrows(), desc=\"Evaluating translations\"):\n",
    "            original = row['English_code']\n",
    "            hindi = row['Hindi_code']\n",
    "            \n",
    "            # Round-trip translation\n",
    "            back_translated = self.reverse_translate_code(hindi)\n",
    "            \n",
    "            # Calculate BLEU score\n",
    "            bleu_score = self.calculate_bleu(original, back_translated)\n",
    "            \n",
    "            results.append({\n",
    "                'original': original,\n",
    "                'hindi': hindi,\n",
    "                'back_translated': back_translated,\n",
    "                'bleu_score': bleu_score\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Usage example\n",
    "def run_evaluation():\n",
    "    config = Config()\n",
    "    evaluator = TranslationEvaluator(config)\n",
    "    \n",
    "    # Load translations\n",
    "    df = pd.read_csv('data/translations_gpt.csv')\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.evaluate_translations(df)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv('data/evaluation_results.csv', index=False)\n",
    "    \n",
    "    # Print average BLEU score\n",
    "    avg_bleu = results['bleu_score'].mean()\n",
    "    print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import tokenize\n",
    "from io import StringIO\n",
    "\n",
    "class SyntaxValidator:\n",
    "    def __init__(self):\n",
    "        self.errors = []\n",
    "    \n",
    "    def validate_syntax(self, code: str) -> bool:\n",
    "        \"\"\"Check if the code is syntactically valid Python\"\"\"\n",
    "        try:\n",
    "            ast.parse(code)\n",
    "            return True\n",
    "        except SyntaxError as e:\n",
    "            self.errors.append(f\"Syntax error: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def validate_token_structure(self, original: str, translated: str) -> float:\n",
    "        \"\"\"Compare token structure between original and translated code\"\"\"\n",
    "        def get_token_types(code):\n",
    "            try:\n",
    "                tokens = []\n",
    "                for tok in tokenize.generate_tokens(StringIO(code).readline):\n",
    "                    tokens.append(tok.type)\n",
    "                return tokens\n",
    "            except tokenize.TokenError:\n",
    "                return []\n",
    "        \n",
    "        original_tokens = get_token_types(original)\n",
    "        translated_tokens = get_token_types(translated)\n",
    "        \n",
    "        if not original_tokens or not translated_tokens:\n",
    "            return 0.0\n",
    "            \n",
    "        # Calculate token structure similarity\n",
    "        min_len = min(len(original_tokens), len(translated_tokens))\n",
    "        matches = sum(1 for i in range(min_len) if original_tokens[i] == translated_tokens[i])\n",
    "        \n",
    "        return matches / max(len(original_tokens), len(translated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "import timeout_decorator\n",
    "import random\n",
    "\n",
    "class SemanticTester:\n",
    "    def __init__(self, timeout_seconds=2):\n",
    "        self.timeout_seconds = timeout_seconds\n",
    "    \n",
    "    @timeout_decorator.timeout(2)\n",
    "    def safe_execute(self, code: str, test_inputs: list) -> list:\n",
    "        \"\"\"Safely execute code with test inputs and capture output\"\"\"\n",
    "        namespace = {}\n",
    "        outputs = []\n",
    "        \n",
    "        for test_input in test_inputs:\n",
    "            output = StringIO()\n",
    "            sys.stdin = StringIO(str(test_input))\n",
    "            with redirect_stdout(output):\n",
    "                try:\n",
    "                    exec(code, namespace)\n",
    "                    outputs.append(output.getvalue().strip())\n",
    "                except Exception as e:\n",
    "                    outputs.append(f\"Error: {str(e)}\")\n",
    "                \n",
    "        return outputs\n",
    "    \n",
    "    def generate_test_inputs(self, code: str) -> list:\n",
    "        \"\"\"Generate relevant test inputs based on code analysis\"\"\"\n",
    "        inputs = []\n",
    "        \n",
    "        # Basic inputs\n",
    "        inputs.extend([\n",
    "            \"\",  # Empty input\n",
    "            \"test\",  # String input\n",
    "            \"42\",  # Numeric input\n",
    "            \"True\",  # Boolean input\n",
    "            \"1 2 3\",  # Multiple values\n",
    "        ])\n",
    "        \n",
    "        # Add random inputs\n",
    "        inputs.extend([\n",
    "            str(random.randint(-100, 100)) for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def compare_outputs(self, original_outputs: list, translated_outputs: list) -> float:\n",
    "        \"\"\"Compare outputs and return similarity score\"\"\"\n",
    "        if not original_outputs or not translated_outputs:\n",
    "            return 0.0\n",
    "            \n",
    "        matches = sum(1 for o, t in zip(original_outputs, translated_outputs) if o == t)\n",
    "        return matches / max(len(original_outputs), len(translated_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "class ComprehensiveEvaluator:\n",
    "    def __init__(self):\n",
    "        self.syntax_validator = SyntaxValidator()\n",
    "        self.semantic_tester = SemanticTester()\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])\n",
    "        \n",
    "    def evaluate_translation(self, original_code: str, translated_code: str) -> Dict:\n",
    "        \"\"\"Run comprehensive evaluation of code translation\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Syntax validation\n",
    "        results['syntax_valid'] = self.syntax_validator.validate_syntax(translated_code)\n",
    "        results['token_similarity'] = self.syntax_validator.validate_token_structure(\n",
    "            original_code, translated_code\n",
    "        )\n",
    "        \n",
    "        # Semantic testing\n",
    "        test_inputs = self.semantic_tester.generate_test_inputs(original_code)\n",
    "        try:\n",
    "            original_outputs = self.semantic_tester.safe_execute(original_code, test_inputs)\n",
    "            translated_outputs = self.semantic_tester.safe_execute(translated_code, test_inputs)\n",
    "            results['semantic_similarity'] = self.semantic_tester.compare_outputs(\n",
    "                original_outputs, translated_outputs\n",
    "            )\n",
    "        except Exception as e:\n",
    "            results['semantic_similarity'] = 0.0\n",
    "            results['semantic_error'] = str(e)\n",
    "        \n",
    "        # Natural language metrics for comments and identifiers\n",
    "        def extract_comments_and_identifiers(code: str) -> Tuple[List[str], List[str]]:\n",
    "            comments = []\n",
    "            identifiers = []\n",
    "            # Basic extraction - could be enhanced with proper parsing\n",
    "            for line in code.split('\\n'):\n",
    "                if '#' in line:\n",
    "                    comments.append(line.split('#')[1].strip())\n",
    "                # Extract potential identifiers\n",
    "                words = line.split()\n",
    "                identifiers.extend([w for w in words if w.isidentifier()])\n",
    "            return comments, identifiers\n",
    "        \n",
    "        orig_comments, orig_ids = extract_comments_and_identifiers(original_code)\n",
    "        trans_comments, trans_ids = extract_comments_and_identifiers(translated_code)\n",
    "        \n",
    "        if orig_comments and trans_comments:\n",
    "            results['comment_meteor'] = meteor_score([orig_comments], trans_comments)\n",
    "            rouge_scores = self.rouge_scorer.score(\n",
    "                '\\n'.join(orig_comments),\n",
    "                '\\n'.join(trans_comments)\n",
    "            )\n",
    "            results['comment_rouge'] = {\n",
    "                k: v.fmeasure for k, v in rouge_scores.items()\n",
    "            }\n",
    "        \n",
    "        if orig_ids and trans_ids:\n",
    "            results['identifier_bleu'] = sentence_bleu([orig_ids], trans_ids)\n",
    "        \n",
    "        # Calculate overall score\n",
    "        valid_scores = [\n",
    "            score for score in [\n",
    "                results.get('token_similarity'),\n",
    "                results.get('semantic_similarity'),\n",
    "                results.get('comment_meteor', 0),\n",
    "                results.get('identifier_bleu', 0)\n",
    "            ] if score is not None\n",
    "        ]\n",
    "        \n",
    "        results['overall_score'] = np.mean(valid_scores) if valid_scores else 0.0\n",
    "        \n",
    "        return results\n",
    "\n",
    "def evaluate_translations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate all translations in the dataset\"\"\"\n",
    "    evaluator = ComprehensiveEvaluator()\n",
    "    results = []\n",
    "    \n",
    "    for _, row in tqdm(df.iterrows(), desc=\"Evaluating translations\"):\n",
    "        evaluation = evaluator.evaluate_translation(\n",
    "            row['original'],\n",
    "            row['back_translated']\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'original': row['original'],\n",
    "            'translated': row['back_translated'],\n",
    "            **evaluation\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating translations: 5it [00:51, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Summary:\n",
      "Average Overall Score: 0.5999\n",
      "Syntax Valid Rate: 1.0000\n",
      "Average Semantic Similarity: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your translations\n",
    "df = pd.read_csv('data/evaluation_results.csv')\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "evaluation_results = evaluate_translations(df)\n",
    "\n",
    "# Save detailed results\n",
    "evaluation_results.to_csv('data/comprehensive_evaluation_results.csv', index=False)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(f\"Average Overall Score: {evaluation_results['overall_score'].mean():.4f}\")\n",
    "print(f\"Syntax Valid Rate: {evaluation_results['syntax_valid'].mean():.4f}\")\n",
    "print(f\"Average Semantic Similarity: {evaluation_results['semantic_similarity'].mean():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
